{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a5d6c4f-a963-4e2e-a694-f3fa2019700f",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca79326-e02f-43c0-b3a7-b239d96d3d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'xcodec_mini_infer'))\n",
    "sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'xcodec_mini_infer', 'descriptaudiocodec'))\n",
    "from models.soundstream_hubert_new import SoundStream\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.transforms import Resample\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f6aa2c-6f46-41fd-aafe-6eda03b3e5b7",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6977da-0ad7-4507-bd0e-f4c76f588da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_mono(filepath, sampling_rate=16000):\n",
    "    audio, sr = torchaudio.load(filepath)\n",
    "    # Convert to mono\n",
    "    audio = torch.mean(audio, dim=0, keepdim=True)\n",
    "    # Resample if needed\n",
    "    if sr != sampling_rate:\n",
    "        resampler = Resample(orig_freq=sr, new_freq=sampling_rate)\n",
    "        audio = resampler(audio)\n",
    "    return audio\n",
    "\n",
    "def encode_audio(codec_model, audio_prompt, device, target_bw=0.5):\n",
    "    if len(audio_prompt.shape) < 3:\n",
    "        audio_prompt.unsqueeze_(0)\n",
    "    with torch.no_grad():\n",
    "        raw_codes = codec_model.encode(audio_prompt.to(device), target_bw=target_bw)\n",
    "    raw_codes = raw_codes.transpose(0, 1)\n",
    "    raw_codes = raw_codes.cpu().numpy().astype(np.int16)\n",
    "    return raw_codes[0]\n",
    "\n",
    "\n",
    "def save_audio(wav: torch.Tensor, path, sample_rate: int, rescale: bool = False):\n",
    "    folder_path = os.path.dirname(path)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    limit = 0.99\n",
    "    max_val = wav.abs().max()\n",
    "    wav = wav * min(limit / max_val, 1) if rescale else wav.clamp(-limit, limit)\n",
    "    torchaudio.save(str(path), wav, sample_rate=sample_rate, encoding='PCM_S', bits_per_sample=16)\n",
    "    \n",
    "#bottom level/first layer encoding. This is sufficient since we don't need to train stage 2 model\n",
    "def encode(audio_path, code_dir_path, codec_model, device):\n",
    "    audio_data = load_audio_mono(audio_path)\n",
    "    raw_codes = encode_audio(codec_model, audio_data, device, target_bw=0.5)\n",
    "    code_file_name = os.path.splitext(os.path.basename(audio_path))[0] + \".npy\"\n",
    "    #dimension of the codes is (1, 1, n). We want to go out a level\n",
    "    np.save(os.path.join(code_dir_path, code_file_name), raw_codes[0])\n",
    "    print(f\"Encoding of {audio_path} finished.\")\n",
    "\n",
    "#no upsampling\n",
    "def decode(npy, save_path, codec_model, device):\n",
    "    tracks = []\n",
    "    codec_result = np.load(npy)\n",
    "    decodec_rlt=[]\n",
    "    with torch.no_grad():\n",
    "        decoded_waveform = codec_model.decode(torch.as_tensor(codec_result.astype(np.int16), dtype=torch.long).unsqueeze(0).permute(1, 0, 2).to(device))\n",
    "    decoded_waveform = decoded_waveform.cpu().squeeze(0)\n",
    "    decodec_rlt.append(torch.as_tensor(decoded_waveform))\n",
    "    decodec_rlt = torch.cat(decodec_rlt, dim=-1)\n",
    "    tracks.append(save_path)\n",
    "    save_audio(decodec_rlt, save_path, 16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1badb3-7f65-41da-aa21-b3abf6e57434",
   "metadata": {},
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c57ee08-3212-426a-b860-cb19bc3edbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise model\n",
    "cuda_idx = 0\n",
    "device = torch.device(f\"cuda:{cuda_idx}\" if torch.cuda.is_available() else \"cpu\")\n",
    "basic_model_config = \"./xcodec_mini_infer/final_ckpt/config.yaml\"\n",
    "resume_path = \"./xcodec_mini_infer/final_ckpt/ckpt_00360000.pth\"\n",
    "model_config = OmegaConf.load(basic_model_config)\n",
    "codec_model = eval(model_config.generator.name)(**model_config.generator.config).to(device)\n",
    "parameter_dict = torch.load(resume_path, map_location='cpu', weights_only=False)\n",
    "codec_model.load_state_dict(parameter_dict['codec_model'])\n",
    "codec_model.to(device)\n",
    "codec_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a014b8f2-2896-4ad1-a97e-a54eef014262",
   "metadata": {},
   "source": [
    "# Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a95045a-60f8-48b4-9c78-30d46c5ef73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "def parallel_encode(audio_dir_path, code_dir_path, codec_model, device);\n",
    "     if __name__ == \"__main__\":\n",
    "        audio_paths = [str(file) for file in Path(audio_dir_path).rglob('*.flac') if file.is_file()] #NEED CHANGE TO MP3\n",
    "        audio_file_count = len(audio_paths) \n",
    "        #ProcessPoolExecutor is probably faster, but might not work\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = [\n",
    "                executor.map(encode, audio_paths, [code_dir_path] * audio_file_count, \n",
    "                             [codec_model] * audio_file_count, [device] * audio_file_count)\n",
    "            ]\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                print(result)\n",
    "    \n",
    "#encode\n",
    "audio_dir_path = \"/homes/al4624/Documents/YuE_finetune/test_sep_original/\"\n",
    "code_dir_path = \"/homes/al4624/Documents/YuE_finetune/test_sep_original/\"\n",
    "parallel_encode(audio_dir_path, code_dir_path, codec_model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab89b2d-66f7-4bd6-8728-a20d010ae82a",
   "metadata": {},
   "source": [
    "# Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4500fc-53ff-441b-bfbc-51c980a79261",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode\n",
    "# reconstruct track\n",
    "npy = \"/homes/al4624/Documents/YuE_finetune/YuE_finetune_trans_gen/finetune/example/npy/dummy.npy\"\n",
    "save_path = \"/homes/al4624/Documents/YuE_finetune/test_sep_original/test_reconstructed.mp3\"\n",
    "decode(npy, save_path, codec_model, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
